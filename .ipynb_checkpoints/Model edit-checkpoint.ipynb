{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af59945a-36e2-482b-9c58-9bfc87f0a483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations: 5\n",
      "Sea level shape (time x stations): (622392, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>station_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sea_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950-01-01 00:00:00.000000</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950-01-01 00:59:59.999997</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1950-01-01 02:00:00.000003</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950-01-01 03:00:00.000000</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1950-01-01 03:59:59.999997</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time station_name  latitude  longitude  sea_level\n",
       "0 1950-01-01 00:00:00.000000    Annapolis  38.98328   -76.4816      1.341\n",
       "1 1950-01-01 00:59:59.999997    Annapolis  38.98328   -76.4816      1.311\n",
       "2 1950-01-01 02:00:00.000003    Annapolis  38.98328   -76.4816      1.280\n",
       "3 1950-01-01 03:00:00.000000    Annapolis  38.98328   -76.4816      1.280\n",
       "4 1950-01-01 03:59:59.999997    Annapolis  38.98328   -76.4816      1.341"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# ============================================\n",
    "# Install & Import Dependencies\n",
    "# ============================================\n",
    "# !pip install scipy pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================\n",
    "# Helper Function: MATLAB datenum → datetime\n",
    "# ============================================\n",
    "def matlab2datetime(matlab_datenum):\n",
    "    return datetime.fromordinal(int(matlab_datenum)) \\\n",
    "           + timedelta(days=matlab_datenum % 1) \\\n",
    "           - timedelta(days=366)\n",
    "\n",
    "# ============================================s\n",
    "# Load .mat Dataset\n",
    "# ============================================\n",
    "data = loadmat('NEUSTG_19502020_12stations.mat')\n",
    "\n",
    "lat = data['lattg'].flatten()\n",
    "lon = data['lontg'].flatten()\n",
    "sea_level = data['sltg']\n",
    "station_names = [s[0] for s in data['sname'].flatten()]\n",
    "time = data['t'].flatten()\n",
    "time_dt = np.array([matlab2datetime(t) for t in time])\n",
    "\n",
    "# ============================================\n",
    "# Select Target Stations\n",
    "# ============================================\n",
    "SELECTED_STATIONS = [\n",
    "    'Annapolis', 'Atlantic_City', 'Charleston', 'Washington', 'Wilmington'\n",
    "]\n",
    "\n",
    "selected_idx = [station_names.index(st) for st in SELECTED_STATIONS]\n",
    "selected_names = [station_names[i] for i in selected_idx]\n",
    "selected_lat = lat[selected_idx]\n",
    "selected_lon = lon[selected_idx]\n",
    "selected_sea_level = sea_level[:, selected_idx]  # time × selected_stations\n",
    "\n",
    "# ============================================\n",
    "# Build Preview DataFrame\n",
    "# ============================================\n",
    "df_preview = pd.DataFrame({\n",
    "    'time': np.tile(time_dt[:5], len(selected_names)),\n",
    "    'station_name': np.repeat(selected_names, 5),\n",
    "    'latitude': np.repeat(selected_lat, 5),\n",
    "    'longitude': np.repeat(selected_lon, 5),\n",
    "    'sea_level': selected_sea_level[:5, :].T.flatten()\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# Print Data Head\n",
    "# ============================================\n",
    "print(f\"Number of stations: {len(selected_names)}\")\n",
    "print(f\"Sea level shape (time x stations): {selected_sea_level.shape}\")\n",
    "df_preview.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba30fb02-18f1-4fdd-9db6-b6e63692ff7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>time</th>\n",
       "      <th>sea_level</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>flood_threshold</th>\n",
       "      <th>sea_level_max</th>\n",
       "      <th>flood</th>\n",
       "      <th>sea_level_3d_mean</th>\n",
       "      <th>sea_level_7d_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1.471958</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>2.396988</td>\n",
       "      <td>2.067</td>\n",
       "      <td>0</td>\n",
       "      <td>1.471958</td>\n",
       "      <td>1.471958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>1.455417</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>2.396988</td>\n",
       "      <td>2.505</td>\n",
       "      <td>1</td>\n",
       "      <td>1.463687</td>\n",
       "      <td>1.463687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-03</td>\n",
       "      <td>1.841542</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>2.396988</td>\n",
       "      <td>2.536</td>\n",
       "      <td>1</td>\n",
       "      <td>1.589639</td>\n",
       "      <td>1.589639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-04</td>\n",
       "      <td>1.396750</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>2.396988</td>\n",
       "      <td>1.737</td>\n",
       "      <td>0</td>\n",
       "      <td>1.564569</td>\n",
       "      <td>1.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-05</td>\n",
       "      <td>1.704333</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>2.396988</td>\n",
       "      <td>2.292</td>\n",
       "      <td>0</td>\n",
       "      <td>1.647542</td>\n",
       "      <td>1.574000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_name       time  sea_level  latitude  longitude  flood_threshold  \\\n",
       "0    Annapolis 1950-01-01   1.471958  38.98328   -76.4816         2.396988   \n",
       "1    Annapolis 1950-01-02   1.455417  38.98328   -76.4816         2.396988   \n",
       "2    Annapolis 1950-01-03   1.841542  38.98328   -76.4816         2.396988   \n",
       "3    Annapolis 1950-01-04   1.396750  38.98328   -76.4816         2.396988   \n",
       "4    Annapolis 1950-01-05   1.704333  38.98328   -76.4816         2.396988   \n",
       "\n",
       "   sea_level_max  flood  sea_level_3d_mean  sea_level_7d_mean  \n",
       "0          2.067      0           1.471958           1.471958  \n",
       "1          2.505      1           1.463687           1.463687  \n",
       "2          2.536      1           1.589639           1.589639  \n",
       "3          1.737      0           1.564569           1.541417  \n",
       "4          2.292      0           1.647542           1.574000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# ============================================\n",
    "# Convert Hourly → Daily per Station\n",
    "# ============================================\n",
    "# Convert time to pandas datetime\n",
    "time_dt = pd.to_datetime(time_dt)\n",
    "\n",
    "# Build hourly DataFrame for selected stations\n",
    "df_hourly = pd.DataFrame({\n",
    "    'time': np.tile(time_dt, len(selected_names)),\n",
    "    'station_name': np.repeat(selected_names, len(time_dt)),\n",
    "    'latitude': np.repeat(selected_lat, len(time_dt)),\n",
    "    'longitude': np.repeat(selected_lon, len(time_dt)),\n",
    "    'sea_level': selected_sea_level.flatten()\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# Compute Flood Threshold per Station\n",
    "# ============================================\n",
    "threshold_df = df_hourly.groupby('station_name')['sea_level'].agg(['mean','std']).reset_index()\n",
    "threshold_df['flood_threshold'] = threshold_df['mean'] + 1.5 * threshold_df['std']\n",
    "\n",
    "df_hourly = df_hourly.merge(threshold_df[['station_name','flood_threshold']], on='station_name', how='left')\n",
    "\n",
    "# ============================================\n",
    "# Daily Aggregation + Flood Flag\n",
    "# ============================================\n",
    "df_daily = df_hourly.groupby(['station_name', pd.Grouper(key='time', freq='D')]).agg({\n",
    "    'sea_level': 'mean',\n",
    "    'latitude': 'first',\n",
    "    'longitude': 'first',\n",
    "    'flood_threshold': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Flood flag: 1 if any hourly value exceeded threshold that day\n",
    "hourly_max = df_hourly.groupby(['station_name', pd.Grouper(key='time', freq='D')])['sea_level'].max().reset_index()\n",
    "df_daily = df_daily.merge(hourly_max, on=['station_name','time'], suffixes=('','_max'))\n",
    "df_daily['flood'] = (df_daily['sea_level_max'] > df_daily['flood_threshold']).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# Feature Engineering (3d & 7d means)\n",
    "# ============================================\n",
    "df_daily['sea_level_3d_mean'] = df_daily.groupby('station_name')['sea_level'].transform(\n",
    "    lambda x: x.rolling(3, min_periods=1).mean())\n",
    "df_daily['sea_level_7d_mean'] = df_daily.groupby('station_name')['sea_level'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).mean())\n",
    "\n",
    "# Preview\n",
    "df_daily.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507ab71-2d8f-4061-ab9e-23bd90ad4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Using device: cpu\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# ============================================\n",
    "# LSTM Approach with PyTorch\n",
    "# ============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Reshape Data for LSTM (Sequence Format)\n",
    "# ============================================\n",
    "FEATURES = ['sea_level', 'sea_level_3d_mean', 'sea_level_7d_mean']\n",
    "HIST_DAYS = 7\n",
    "FUTURE_DAYS = 14\n",
    "\n",
    "X_train_lstm, y_train_lstm = [], []\n",
    "\n",
    "for stn, grp in df_daily.groupby('station_name'):\n",
    "    grp = grp.sort_values('time').reset_index(drop=True)\n",
    "    for i in range(len(grp) - HIST_DAYS - FUTURE_DAYS):\n",
    "        # Keep sequence structure: (7 days, 3 features)\n",
    "        hist = grp.loc[i:i+HIST_DAYS-1, FEATURES].values  # Shape: (7, 3)\n",
    "        future = grp.loc[i+HIST_DAYS:i+HIST_DAYS+FUTURE_DAYS-1, 'flood'].values\n",
    "        X_train_lstm.append(hist)\n",
    "        y_train_lstm.append(future)\n",
    "\n",
    "X_train_lstm = np.array(X_train_lstm)  # Shape: (samples, 7, 3)\n",
    "y_train_lstm = np.array(y_train_lstm)  # Shape: (samples, 14)\n",
    "\n",
    "print(f\"LSTM Training Data Shapes:\")\n",
    "print(f\"X_train_lstm: {X_train_lstm.shape} (samples, sequence_length, features)\")\n",
    "print(f\"y_train_lstm: {y_train_lstm.shape} (samples, future_days)\\n\")\n",
    "\n",
    "# Normalize features for LSTM\n",
    "scaler = StandardScaler()\n",
    "n_samples, n_timesteps, n_features = X_train_lstm.shape\n",
    "X_train_lstm_reshaped = X_train_lstm.reshape(-1, n_features)\n",
    "X_train_lstm_scaled = scaler.fit_transform(X_train_lstm_reshaped)\n",
    "X_train_lstm = X_train_lstm_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "# Prepare test data for LSTM\n",
    "X_test_lstm = []\n",
    "for stn, grp in df_daily.groupby('station_name'):\n",
    "    mask = (grp['time'] >= hist_start) & (grp['time'] <= hist_end)\n",
    "    hist_block = grp.loc[mask, FEATURES].values\n",
    "    if len(hist_block) == 7:  # ensure full 7-day block\n",
    "        X_test_lstm.append(hist_block)\n",
    "\n",
    "X_test_lstm = np.array(X_test_lstm)  # Shape: (stations, 7, 3)\n",
    "\n",
    "# Normalize test data with same scaler\n",
    "n_test_samples, n_test_timesteps, n_test_features = X_test_lstm.shape\n",
    "X_test_lstm_reshaped = X_test_lstm.reshape(-1, n_test_features)\n",
    "X_test_lstm_scaled = scaler.transform(X_test_lstm_reshaped)\n",
    "X_test_lstm = X_test_lstm_scaled.reshape(n_test_samples, n_test_timesteps, n_test_features)\n",
    "\n",
    "print(f\"X_test_lstm shape: {X_test_lstm.shape}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# PyTorch Dataset Class\n",
    "# ============================================\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = FloodDataset(X_train_lstm, y_train_lstm)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# ============================================\n",
    "# LSTM Model Architecture\n",
    "# ============================================\n",
    "class FloodLSTM(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=64, num_layers=4, output_size=14, dropout=0.2):\n",
    "        super(FloodLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, sequence_length, features)\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use the last timestep output\n",
    "        last_output = lstm_out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(last_output)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)  # Output probabilities for each of 14 days\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ============================================\n",
    "# Initialize Model, Loss, Optimizer\n",
    "# ============================================\n",
    "model = FloodLSTM(\n",
    "    input_size=3,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=14,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Training Loop\n",
    "# ============================================\n",
    "num_epochs = 30\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting LSTM Training...\")\n",
    "print(f\"{'Epoch':<8} {'Train Loss':<12} {'LR':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"{epoch+1:<8} {avg_loss:<12.6f} {current_lr:<10.6f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# ============================================\n",
    "# LSTM Predictions\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LSTM Predictions\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test_lstm).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_lstm = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_lstm_bin = (y_pred_lstm > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Predictions shape: {y_pred_lstm.shape}\")\n",
    "print(f\"Prediction probabilities (first station, first 5 days): {y_pred_lstm[0, :5]}\")\n",
    "print(f\"Binary predictions (first station, first 5 days): {y_pred_lstm_bin[0, :5]}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# LSTM Evaluation\n",
    "# ============================================\n",
    "y_true_flat_lstm = y_true.flatten()\n",
    "y_pred_flat_lstm = y_pred_lstm_bin.flatten()\n",
    "\n",
    "tn_lstm, fp_lstm, fn_lstm, tp_lstm = confusion_matrix(y_true_flat_lstm, y_pred_flat_lstm).ravel()\n",
    "acc_lstm = accuracy_score(y_true_flat_lstm, y_pred_flat_lstm)\n",
    "f1_lstm = f1_score(y_true_flat_lstm, y_pred_flat_lstm)\n",
    "mcc_lstm = matthews_corrcoef(y_true_flat_lstm, y_pred_flat_lstm)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"LSTM Results\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(f\"TP: {tp_lstm} | FP: {fp_lstm} | TN: {tn_lstm} | FN: {fn_lstm}\")\n",
    "print(\"\\n=== Metrics ===\")\n",
    "print(f\"Accuracy: {acc_lstm:.3f}\")\n",
    "print(f\"F1 Score: {f1_lstm:.3f}\")\n",
    "print(f\"MCC: {mcc_lstm:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# Comparison: XGBoost vs LSTM\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Comparison: XGBoost vs LSTM\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'Metric':<15} {'XGBoost':<12} {'LSTM':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy':<15} {acc:<12.3f} {acc_lstm:<12.3f} {acc_lstm-acc:<+12.3f}\")\n",
    "print(f\"{'F1 Score':<15} {f1:<12.3f} {f1_lstm:<12.3f} {f1_lstm-f1:<+12.3f}\")\n",
    "print(f\"{'MCC':<15} {mcc:<12.3f} {mcc_lstm:<12.3f} {mcc_lstm-mcc:<+12.3f}\")\n",
    "print(f\"\\n{'True Positives':<15} {tp:<12} {tp_lstm:<12} {tp_lstm-tp:<+12}\")\n",
    "print(f\"{'False Positives':<15} {fp:<12} {fp_lstm:<12} {fp_lstm-fp:<+12}\")\n",
    "print(f\"{'True Negatives':<15} {tn:<12} {tn_lstm:<12} {tn_lstm-tn:<+12}\")\n",
    "print(f\"{'False Negatives':<15} {fn:<12} {fn_lstm:<12} {fn_lstm-fn:<+12}\")\n",
    "\n",
    "# Save LSTM predictions for further analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Saved files:\")\n",
    "print(\"  - best_lstm_model.pth (PyTorch model state)\")\n",
    "print(\"=\"*50)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d10cd5-b2b3-48ac-99ea-7efa4a7b2846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n",
      "\n",
      "Loading dataset...\n",
      "Stations selected: 8\n",
      "Timesteps: 622392\n",
      "Dropping 0 fully-missing days\n",
      "Feature tensor shape: (25933, 8, 3)\n",
      "Label tensor shape: (25933, 8)\n",
      "FloodLSTM(\n",
      "  (lstm): LSTM(3, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=32, out_features=14, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Total parameters: 53,486\n",
      "\n",
      "Starting Training\n",
      "\n",
      "Epoch   Loss        LR        \n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_175/3054467371.py:107: RuntimeWarning: Mean of empty slice\n",
      "  daily_mean = np.nanmean(daily_blocks, axis=1)\n",
      "/tmp/ipykernel_175/3054467371.py:108: RuntimeWarning: All-NaN slice encountered\n",
      "  daily_max = np.nanmax(daily_blocks, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       0.617118    0.001000  \n",
      "2       0.612378    0.001000  \n",
      "3       0.603752    0.001000  \n",
      "4       0.598819    0.001000  \n",
      "5       0.597075    0.001000  \n",
      "6       0.596017    0.001000  \n",
      "7       0.594999    0.001000  \n",
      "8       0.594116    0.001000  \n",
      "9       0.593414    0.001000  \n",
      "10      0.592931    0.001000  \n",
      "11      0.592251    0.001000  \n",
      "12      0.591634    0.001000  \n",
      "13      0.591219    0.001000  \n",
      "14      0.589012    0.001000  \n",
      "15      0.585901    0.001000  \n",
      "16      0.583813    0.001000  \n",
      "17      0.582318    0.001000  \n",
      "18      0.581714    0.001000  \n",
      "19      0.581125    0.001000  \n",
      "20      0.580613    0.001000  \n",
      "21      0.580100    0.001000  \n",
      "22      0.579459    0.001000  \n",
      "23      0.579241    0.001000  \n",
      "24      0.578699    0.001000  \n",
      "25      0.578529    0.001000  \n",
      "26      0.578148    0.001000  \n",
      "27      0.577934    0.001000  \n",
      "28      0.577642    0.001000  \n",
      "29      0.577514    0.001000  \n",
      "30      0.577422    0.001000  \n",
      "\n",
      "Loading best model...\n",
      "\n",
      "=== LSTM Results ===\n",
      "TP: 14 | FP: 4 | TN: 46 | FN: 48\n",
      "Accuracy: 0.536\n",
      "F1 Score: 0.350\n",
      "MCC: 0.197\n",
      "ROC AUC: 0.623\n",
      "\n",
      "Saved files:\n",
      " - best_lstm_model.pth\n",
      " - roc_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_175/3054467371.py:308: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# ============================================\n",
    "# Flood Prediction LSTM - Research Pipeline\n",
    "# Memory Efficient + Normalization + ROC/AUC\n",
    "# NaN Safe (Drop missing days + Interpolate)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# Config\n",
    "# ============================================\n",
    "DATA_FILE = \"NEUSTG_19502020_12stations.mat\"\n",
    "\n",
    "SELECTED_STATIONS = [\n",
    "    \"Annapolis\", \"Atlantic_City\", \"Charleston\", \"Washington\", \"Wilmington\", \"Eastport\", \"Fernandina_Beach\", \"Lewes\"\n",
    "]\n",
    "\n",
    "HIST_DAYS = 7\n",
    "FUTURE_DAYS = 14\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "PATIENCE = 10\n",
    "\n",
    "# ============================================\n",
    "# Device\n",
    "# ============================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# MATLAB Time Conversion\n",
    "# ============================================\n",
    "def matlab2datetime(matlab_datenum):\n",
    "    return datetime.fromordinal(int(matlab_datenum)) \\\n",
    "        + timedelta(days=matlab_datenum % 1) \\\n",
    "        - timedelta(days=366)\n",
    "\n",
    "# ============================================\n",
    "# Load Dataset\n",
    "# ============================================\n",
    "print(\"Loading dataset...\")\n",
    "data = loadmat(DATA_FILE)\n",
    "\n",
    "lat = data[\"lattg\"].flatten()\n",
    "lon = data[\"lontg\"].flatten()\n",
    "sea_level = data[\"sltg\"].astype(np.float32)  # (time, stations)\n",
    "station_names = [s[0] for s in data[\"sname\"].flatten()]\n",
    "time_raw = data[\"t\"].flatten()\n",
    "time_dt = np.array([matlab2datetime(t) for t in time_raw])\n",
    "\n",
    "# ============================================\n",
    "# Select Stations\n",
    "# ============================================\n",
    "selected_idx = [station_names.index(st) for st in SELECTED_STATIONS]\n",
    "\n",
    "sea_level = sea_level[:, selected_idx]\n",
    "lat = lat[selected_idx]\n",
    "lon = lon[selected_idx]\n",
    "\n",
    "n_time, n_stations = sea_level.shape\n",
    "\n",
    "print(f\"Stations selected: {n_stations}\")\n",
    "print(f\"Timesteps: {n_time}\")\n",
    "\n",
    "# ============================================\n",
    "# Flood Threshold (Vectorized)\n",
    "# ============================================\n",
    "mean_levels = np.nanmean(sea_level, axis=0)\n",
    "std_levels = np.nanstd(sea_level, axis=0)\n",
    "flood_thresholds = mean_levels + 1.5 * std_levels\n",
    "\n",
    "# ============================================\n",
    "# Hourly → Daily Aggregation\n",
    "# ============================================\n",
    "HOURS_PER_DAY = 24\n",
    "n_days = n_time // HOURS_PER_DAY\n",
    "\n",
    "sea_level = sea_level[:n_days * HOURS_PER_DAY]\n",
    "\n",
    "daily_blocks = sea_level.reshape(\n",
    "    n_days, HOURS_PER_DAY, n_stations\n",
    ")\n",
    "\n",
    "daily_mean = np.nanmean(daily_blocks, axis=1)\n",
    "daily_max = np.nanmax(daily_blocks, axis=1)\n",
    "\n",
    "# ============================================\n",
    "# Handle Missing Days (NaN Safety)\n",
    "# ============================================\n",
    "\n",
    "# Drop days where ALL stations are NaN\n",
    "bad_days = np.all(np.isnan(daily_mean), axis=1)\n",
    "print(f\"Dropping {bad_days.sum()} fully-missing days\")\n",
    "\n",
    "daily_mean = daily_mean[~bad_days]\n",
    "daily_max = daily_max[~bad_days]\n",
    "\n",
    "# Recompute flood labels safely\n",
    "flood_daily = (daily_max > flood_thresholds).astype(np.float32)\n",
    "\n",
    "# Interpolate remaining NaNs per station\n",
    "df_mean = pd.DataFrame(daily_mean)\n",
    "daily_mean = df_mean.interpolate(limit_direction=\"both\").values.astype(np.float32)\n",
    "\n",
    "# ============================================\n",
    "# Feature Engineering\n",
    "# ============================================\n",
    "df_mean = pd.DataFrame(daily_mean)\n",
    "\n",
    "mean_3d = df_mean.rolling(3, min_periods=1).mean().values\n",
    "mean_7d = df_mean.rolling(7, min_periods=1).mean().values\n",
    "\n",
    "FEATURES = np.stack([\n",
    "    daily_mean,\n",
    "    mean_3d,\n",
    "    mean_7d\n",
    "], axis=2).astype(np.float32)  # (days, stations, features)\n",
    "\n",
    "LABELS = flood_daily.astype(np.float32)\n",
    "\n",
    "print(f\"Feature tensor shape: {FEATURES.shape}\")\n",
    "print(f\"Label tensor shape: {LABELS.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# Train/Test Split (Time-based)\n",
    "# ============================================\n",
    "n_days_clean = FEATURES.shape[0]\n",
    "split_day = int(n_days_clean * 0.8)\n",
    "\n",
    "X_train = FEATURES[:split_day]\n",
    "y_train = LABELS[:split_day]\n",
    "\n",
    "X_test = FEATURES[split_day - HIST_DAYS:]\n",
    "y_test = LABELS[split_day:]\n",
    "\n",
    "# ============================================\n",
    "# Normalization (Train Only)\n",
    "# ============================================\n",
    "scaler = StandardScaler()\n",
    "\n",
    "flat_train = X_train.reshape(-1, X_train.shape[2])\n",
    "flat_train = scaler.fit_transform(flat_train)\n",
    "\n",
    "X_train = flat_train.reshape(X_train.shape).astype(np.float32)\n",
    "\n",
    "flat_test = X_test.reshape(-1, X_test.shape[2])\n",
    "flat_test = scaler.transform(flat_test)\n",
    "X_test = flat_test.reshape(X_test.shape).astype(np.float32)\n",
    "\n",
    "# ============================================\n",
    "# Safety Checks\n",
    "# ============================================\n",
    "assert not np.isnan(X_train).any(), \"NaNs in X_train\"\n",
    "assert not np.isnan(y_train).any(), \"NaNs in y_train\"\n",
    "assert not np.isnan(X_test).any(), \"NaNs in X_test\"\n",
    "assert not np.isnan(y_test).any(), \"NaNs in y_test\"\n",
    "\n",
    "# ============================================\n",
    "# Streaming Dataset\n",
    "# ============================================\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, X, y, hist_days, future_days):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.hist = hist_days\n",
    "        self.future = future_days\n",
    "        self.days, self.stations, _ = X.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.days - self.hist - self.future) * self.stations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        stn = idx % self.stations\n",
    "        day = idx // self.stations\n",
    "\n",
    "        x_seq = self.X[\n",
    "            day:day + self.hist,\n",
    "            stn,\n",
    "            :\n",
    "        ]\n",
    "\n",
    "        y_seq = self.y[\n",
    "            day + self.hist:day + self.hist + self.future,\n",
    "            stn\n",
    "        ]\n",
    "\n",
    "        return torch.from_numpy(x_seq), torch.from_numpy(y_seq)\n",
    "\n",
    "train_dataset = FloodDataset(X_train, y_train, HIST_DAYS, FUTURE_DAYS)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ============================================\n",
    "# LSTM Model\n",
    "# ============================================\n",
    "class FloodLSTM(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=128, num_layers=6, output_size=14, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last = lstm_out[:, -1, :]\n",
    "        x = self.fc1(last)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "model = FloodLSTM(\n",
    "    input_size=3,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=FUTURE_DAYS,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# Training Setup\n",
    "# ============================================\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "# ============================================\n",
    "# Training Loop\n",
    "# ============================================\n",
    "best_loss = np.inf\n",
    "pat_counter = 0\n",
    "\n",
    "print(\"\\nStarting Training\\n\")\n",
    "print(f\"{'Epoch':<8}{'Loss':<12}{'LR':<10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb)\n",
    "        loss = criterion(preds, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"{epoch+1:<8}{avg_loss:<12.6f}{lr_now:<10.6f}\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        pat_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_lstm_model.pth\")\n",
    "    else:\n",
    "        pat_counter += 1\n",
    "\n",
    "    if pat_counter >= PATIENCE:\n",
    "        print(\"\\nEarly stopping triggered\")\n",
    "        break\n",
    "\n",
    "# ============================================\n",
    "# Evaluation + ROC\n",
    "# ============================================\n",
    "print(\"\\nLoading best model...\")\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_prob = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for stn in range(X_test.shape[1]):\n",
    "        hist_block = X_test[:HIST_DAYS, stn]\n",
    "        hist_block = torch.from_numpy(hist_block).unsqueeze(0).to(device)\n",
    "\n",
    "        probs = model(hist_block).cpu().numpy()[0]\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        y_prob.extend(probs)\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y_test[HIST_DAYS:HIST_DAYS + FUTURE_DAYS, stn])\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_prob = np.array(y_prob)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"\\n=== LSTM Results ===\")\n",
    "print(f\"TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "print(f\"MCC: {mcc:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# Plot ROC Curve\n",
    "# ============================================\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Flood Prediction LSTM - ROC Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"roc_curve.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\" - best_lstm_model.pth\")\n",
    "print(\" - roc_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52eb898-39f2-4f49-8b3b-251f7d2cad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# ============================================\n",
    "# Flood Prediction LSTM (Full Research Version)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import loadmat\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Settings\n",
    "# ============================================\n",
    "DATA_FILE = \"NEUSTG_19502020_12stations.mat\"\n",
    "SELECTED_STATIONS = [\n",
    "    \"Annapolis\", \"Atlantic_City\", \"Charleston\", \"Washington\", \"Wilmington\"\n",
    "]\n",
    "\n",
    "HIST_DAYS = 7\n",
    "FUTURE_DAYS = 14\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "PATIENCE = 7\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ============================================\n",
    "# Helper: MATLAB datenum → datetime\n",
    "# ============================================\n",
    "def matlab2datetime(matlab_datenum):\n",
    "    return (\n",
    "        datetime.fromordinal(int(matlab_datenum))\n",
    "        + timedelta(days=matlab_datenum % 1)\n",
    "        - timedelta(days=366)\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# Load Dataset\n",
    "# ============================================\n",
    "data = loadmat(DATA_FILE)\n",
    "\n",
    "lat = data[\"lattg\"].flatten()\n",
    "lon = data[\"lontg\"].flatten()\n",
    "sea_level = data[\"sltg\"]\n",
    "station_names = [s[0] for s in data[\"sname\"].flatten()]\n",
    "time = data[\"t\"].flatten()\n",
    "time_dt = np.array([matlab2datetime(t) for t in time])\n",
    "\n",
    "# ============================================\n",
    "# Select Stations\n",
    "# ============================================\n",
    "selected_idx = [station_names.index(st) for st in SELECTED_STATIONS]\n",
    "selected_names = [station_names[i] for i in selected_idx]\n",
    "selected_lat = lat[selected_idx]\n",
    "selected_lon = lon[selected_idx]\n",
    "selected_sea_level = sea_level[:, selected_idx]\n",
    "\n",
    "# ============================================\n",
    "# Build Hourly DataFrame\n",
    "# ============================================\n",
    "time_dt = pd.to_datetime(time_dt)\n",
    "\n",
    "df_hourly = pd.DataFrame({\n",
    "    \"time\": np.tile(time_dt, len(selected_names)),\n",
    "    \"station_name\": np.repeat(selected_names, len(time_dt)),\n",
    "    \"latitude\": np.repeat(selected_lat, len(time_dt)),\n",
    "    \"longitude\": np.repeat(selected_lon, len(time_dt)),\n",
    "    \"sea_level\": selected_sea_level.flatten()\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# Threshold per Station\n",
    "# ============================================\n",
    "threshold_df = (\n",
    "    df_hourly\n",
    "    .groupby(\"station_name\")[\"sea_level\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "threshold_df[\"flood_threshold\"] = (\n",
    "    threshold_df[\"mean\"] + 1.5 * threshold_df[\"std\"]\n",
    ")\n",
    "\n",
    "df_hourly = df_hourly.merge(\n",
    "    threshold_df[[\"station_name\", \"flood_threshold\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Daily Aggregation\n",
    "# ============================================\n",
    "df_daily = (\n",
    "    df_hourly\n",
    "    .groupby([\"station_name\", pd.Grouper(key=\"time\", freq=\"D\")])\n",
    "    .agg({\n",
    "        \"sea_level\": \"mean\",\n",
    "        \"latitude\": \"first\",\n",
    "        \"longitude\": \"first\",\n",
    "        \"flood_threshold\": \"first\"\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "hourly_max = (\n",
    "    df_hourly\n",
    "    .groupby([\"station_name\", pd.Grouper(key=\"time\", freq=\"D\")])[\"sea_level\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_daily = df_daily.merge(\n",
    "    hourly_max,\n",
    "    on=[\"station_name\", \"time\"],\n",
    "    suffixes=(\"\", \"_max\")\n",
    ")\n",
    "\n",
    "df_daily[\"flood\"] = (\n",
    "    df_daily[\"sea_level_max\"] > df_daily[\"flood_threshold\"]\n",
    ").astype(int)\n",
    "\n",
    "# ============================================\n",
    "# Feature Engineering\n",
    "# ============================================\n",
    "df_daily[\"sea_level_3d_mean\"] = (\n",
    "    df_daily.groupby(\"station_name\")[\"sea_level\"]\n",
    "    .transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "df_daily[\"sea_level_7d_mean\"] = (\n",
    "    df_daily.groupby(\"station_name\")[\"sea_level\"]\n",
    "    .transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "df_daily = df_daily.dropna().reset_index(drop=True)\n",
    "\n",
    "# ============================================\n",
    "# Build Sequences\n",
    "# ============================================\n",
    "FEATURES = [\"sea_level\", \"sea_level_3d_mean\", \"sea_level_7d_mean\"]\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for stn, grp in df_daily.groupby(\"station_name\"):\n",
    "    grp = grp.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(grp) - HIST_DAYS - FUTURE_DAYS):\n",
    "        hist = grp.loc[i:i+HIST_DAYS-1, FEATURES].values\n",
    "        future = grp.loc[i+HIST_DAYS:i+HIST_DAYS+FUTURE_DAYS-1, \"flood\"].values\n",
    "\n",
    "        if np.isnan(hist).any() or np.isnan(future).any():\n",
    "            continue\n",
    "\n",
    "        X.append(hist)\n",
    "        y.append(future)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Dataset built:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# ============================================\n",
    "# Train / Validation Split (Time-Aware)\n",
    "# ============================================\n",
    "split_idx = int(0.8 * len(X))\n",
    "\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "\n",
    "X_val = X[split_idx:]\n",
    "y_val = y[split_idx:]\n",
    "\n",
    "# ============================================\n",
    "# Normalization (Train Only!)\n",
    "# ============================================\n",
    "scaler = StandardScaler()\n",
    "\n",
    "n_train, t, f = X_train.shape\n",
    "X_train_2d = X_train.reshape(-1, f)\n",
    "X_val_2d = X_val.reshape(-1, f)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_2d).reshape(n_train, t, f)\n",
    "X_val_scaled = scaler.transform(X_val_2d).reshape(len(X_val), t, f)\n",
    "\n",
    "# ============================================\n",
    "# Dataset Class\n",
    "# ============================================\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    FloodDataset(X_train_scaled, y_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FloodDataset(X_val_scaled, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# LSTM Model\n",
    "# ============================================\n",
    "class FloodLSTM(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=64, num_layers=2, output_size=14):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "model = FloodLSTM().to(DEVICE)\n",
    "\n",
    "# ============================================\n",
    "# Learning Rate Finder\n",
    "# ============================================\n",
    "def lr_finder(model, loader, criterion, optimizer,\n",
    "              start_lr=1e-6, end_lr=1e-1, steps=100):\n",
    "\n",
    "    lrs = np.logspace(np.log10(start_lr), np.log10(end_lr), steps)\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    it = iter(loader)\n",
    "\n",
    "    for lr in lrs:\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "        try:\n",
    "            xb, yb = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            xb, yb = next(it)\n",
    "\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return lrs, losses\n",
    "\n",
    "# ============================================\n",
    "# Run LR Finder\n",
    "# ============================================\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Running LR finder...\")\n",
    "lrs, losses = lr_finder(model, train_loader, criterion, optimizer)\n",
    "\n",
    "best_lr = lrs[np.argmin(losses)]\n",
    "print(f\"Suggested LR: {best_lr:.2e}\")\n",
    "\n",
    "plt.plot(lrs, losses)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Rate Finder\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# Training Setup\n",
    "# ============================================\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "# ============================================\n",
    "# Training Loop\n",
    "# ============================================\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_true.append(yb.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_lstm_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ============================================\n",
    "# Evaluation + ROC\n",
    "# ============================================\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_true = np.vstack(all_true)\n",
    "\n",
    "y_pred_bin = (all_preds > 0.5).astype(int)\n",
    "\n",
    "y_true_flat = all_true.flatten()\n",
    "y_pred_flat = y_pred_bin.flatten()\n",
    "y_prob_flat = all_preds.flatten()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_flat, y_pred_flat).ravel()\n",
    "\n",
    "print(\"\\n=== Final Metrics ===\")\n",
    "print(f\"TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true_flat, y_pred_flat):.3f}\")\n",
    "print(f\"F1 Score: {f1_score(y_true_flat, y_pred_flat):.3f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_true_flat, y_pred_flat):.3f}\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true_flat, y_prob_flat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Flood Prediction ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel saved as: best_lstm_model_2.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
